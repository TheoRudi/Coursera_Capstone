{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modelling \n",
    "In machine learning, classification is considered supervised learning, which means learning where the class-labels are already given in the dataset. It can be binary or multi-class classification.  Since there are only two classes of accident severity given, this report will develop binary classification models. The assumption is that the permutation or combination of all independent features in the dataset will have recurring patterns that ‘predict’ the classes. The classification algorithm will find the common pattern of combinations that correspond to either one of the dependent classes. This has many applications in various fields of business and science ranging from spam, fraud, or churn prediction over handwriting- and face-recognition towards extreme event prediction and medical diagnosis. Common classification algorithms include: \n",
    "- K-Nearest Neighbors,\n",
    "- Decision Tree,\n",
    "- Random Forrest,\n",
    "- Logistic Regression,\n",
    "- Artificial Neural Networks.\n",
    "\n",
    "For modelling and evaluation, dataset will be split into a training and a testing subset. The classification algorithm is trained to find the underlying pattern that predicts the classes only from the training subset, while the testing subset simulates out-of-sample accuracy testing. Since there are no means of anticipating how different ML algorithms perform on a given dataset, data scientists use this as form of controlled experiment in order to discover which algorithm and which hyperparameter result in the most accurate classification model. Therefore, several classification algorithms will be modelled and tested to determine the most appropriate model for predicting collision severity on Seattle’s streets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "mpl.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-23 16:51:10--  https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv\n",
      "Resolving s3.us.cloud-object-storage.appdomain.cloud (s3.us.cloud-object-storage.appdomain.cloud)... 67.228.254.196\n",
      "Connecting to s3.us.cloud-object-storage.appdomain.cloud (s3.us.cloud-object-storage.appdomain.cloud)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 73917638 (70M) [text/csv]\n",
      "Saving to: ‘DataCollision.csv’\n",
      "\n",
      "DataCollision.csv   100%[===================>]  70.49M   460KB/s    in 2m 8s   \n",
      "\n",
      "2020-09-23 16:53:21 (562 KB/s) - ‘DataCollision.csv’ saved [73917638/73917638]\n",
      "\n",
      "(194673, 38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theo/Programs/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -O DataCollision.csv https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv\n",
    "df = pd.read_csv('DataCollision.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import Feature Set\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# package for dataset balancing tools\n",
    "!conda install -c conda-forge imbalanced-learn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pre-processing libraries\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Fog/Smog/Smoke', 'Snowing', 'Wet', 'Sleet/Hail/Freezing Rain', 'Ice', 'LONGITUDE', 'At Intersection (intersection related)', 'LATITUDE', 'Sand/Mud/Dirt', 'Mid-Block (but intersection related)', 'Severe Crosswind', 'Overcast', 'Oil', 'Raining', 'Ramp Junction', 'Clear', 'Unknown Weather', 'At Intersection (but not related to intersection)', 'Blowing Sand/Dirt', 'WEEKEND', 'Unknown Roadcond', 'Driveway Junction', 'Mid-Block (not related to intersection)', 'Dry'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a52c5b834c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0;34m'Severe Crosswind'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sleet/Hail/Freezing Rain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Snowing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m        \u001b[0;34m'Unknown Weather'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dry'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Oil'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sand/Mud/Dirt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m        'Unknown Roadcond', 'Wet', 'WEEKEND']] \n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SEVERITYCODE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programs/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Fog/Smog/Smoke', 'Snowing', 'Wet', 'Sleet/Hail/Freezing Rain', 'Ice', 'LONGITUDE', 'At Intersection (intersection related)', 'LATITUDE', 'Sand/Mud/Dirt', 'Mid-Block (but intersection related)', 'Severe Crosswind', 'Overcast', 'Oil', 'Raining', 'Ramp Junction', 'Clear', 'Unknown Weather', 'At Intersection (but not related to intersection)', 'Blowing Sand/Dirt', 'WEEKEND', 'Unknown Roadcond', 'Driveway Junction', 'Mid-Block (not related to intersection)', 'Dry'] not in index\""
     ]
    }
   ],
   "source": [
    "# balancing the dataset\n",
    "x = df[['LONGITUDE', 'LATITUDE', 'INATTENTIONIND', 'UNDERINFL',\n",
    "       'LIGHTCOND', 'SPEEDING',\n",
    "       'At Intersection (but not related to intersection)',\n",
    "       'At Intersection (intersection related)', 'Driveway Junction',\n",
    "       'Mid-Block (but intersection related)',\n",
    "       'Mid-Block (not related to intersection)', 'Ramp Junction',\n",
    "       'Blowing Sand/Dirt', 'Clear', 'Fog/Smog/Smoke', 'Overcast', 'Raining',\n",
    "       'Severe Crosswind', 'Sleet/Hail/Freezing Rain', 'Snowing',\n",
    "       'Unknown Weather', 'Dry', 'Ice', 'Oil', 'Sand/Mud/Dirt',\n",
    "       'Unknown Roadcond', 'Wet', 'WEEKEND']] \n",
    "y = df['SEVERITYCODE']\n",
    "\n",
    "RUS = RandomUnderSampler(random_state=12) # random_state sets the seed which can be used for reproducible results\n",
    "x_resampled, y_resampled = RUS.fit_resample(x, y)\n",
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data normalization\n",
    "x_prp= preprocessing.StandardScaler().fit(x_resampled).transform(x_resampled)\n",
    "x_prp[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test splitting\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_prp, y_resampled, test_size=0.2)\n",
    "print(\"Train set shape: \", x_train.shape, y_train.shape)\n",
    "print(\"Test set shape: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "K-Nearest Neighbors is a comparatively simple classification algorithm that stores all available cases and classifies a new case based on a similarity to its ‘nearest neighbors’. For instance, if an unknown case is compared to 5 neighboring cases and 3 out of 5 of those neighbors are class 2 while the rest are class 1, the unknown case will be classified as class 2. The distance between cases is usually measured by a standard mathematical formula for distances between points in multidimensional planes (e.g. Minkowski distance, which is also employed here).  KNN models work best when the dataset is balanced and its features have been normalized. The challenge in building an accurate KNN model lies in determining the value of K, namely the numbers of neighbors for comparison. A too small k-value may capture too much noise, while increasing it endlessly will run into diminishing marginal gains for accuracy. Given the large feature set of the training data, this challenge is approached by iterating through the integers 15 to 24 and comparing them against their respective mean accuracy (see graph below). The best mean accuracy is achieved at approximately 0.6060 with k equaling 23. A full evaluation of model performances will be given in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding a good K for the KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "Ks = 25\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "\n",
    "for n in range(15,Ks):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=n).fit(x_train, y_train)\n",
    "    yhat = neigh.predict(x_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k-value over accuracy\n",
    "plt.plot(range(15,Ks),mean_acc[14:25])\n",
    "plt.fill_between(range(15,Ks),mean_acc[14:25] - 1 * std_acc[14:25],mean_acc[14:25] + 1 * std_acc[14:25], alpha=0.10)\n",
    "\n",
    "plt.legend(('Accuracy', '+/- 3xstd'))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print( \"The best general accuracy was at\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build final KNN model\n",
    "k=23\n",
    "KNN= KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\n",
    "y_hat = KNN.predict(x_test)\n",
    "\n",
    "# full evalution\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc1=metrics.accuracy_score(y_test, y_hat)\n",
    "jc1=jaccard_score(y_test, y_hat)\n",
    "fs1=f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision Tree classifier is another of predictive modelling approach in machine learning. The Decision Tree iterates through the features about a case (represented in the branches) to conclusions about the case’s target value (represented in the leaves). In Decision Trees, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. The criterion by which branches eventually lead to pure leaves, where 100% of its cases fall into a single class is called ‘entropy’.  Entropy describes the amount of information disorder, if the sample of cases in a node is completely homogenous (i.e. 100% a single class), then the entropy equals 0. If the sample of cases in a node is completely heterogenous (i.e. split 50-50 between the 2 classes), then entropy equals 1.\n",
    "\n",
    "This process is illustrated below with Decision Tree that has been given a maximum depth of 4 (see tree diagram). \n",
    "\n",
    "Without a maximum depth, the Decision Tree algorithm will run until all leaves have become are pure. This has led to a general accuracy score of 0.5719 on the given feature set for predicting severity classes of collisions. The advantages of Decision Trees are that they work well with categorical data and are easy to interpret (as they mirror human decision making). There disadvantages are that they can become overly complex quickly and small changes in the training data can upset their whole structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DTC = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "DTC.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for visualizing decision trees\n",
    "!pip install graphviz\n",
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing decision tree\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "dot_data = tree.export_graphviz(DTC,\n",
    "                                out_file=None, \n",
    "                                feature_names=x.columns,  \n",
    "                                class_names=np.unique(y).astype('str'),  \n",
    "                                filled=True,\n",
    "                                rounded=True,  \n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build full decision tree classifier\n",
    "DTC = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "DTC.fit(x_train, y_train)\n",
    "\n",
    "y_hat = DTC.predict(x_test)\n",
    "print(\"Decision Tree General Accuracy Score: \", metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full evaluation\n",
    "acc2=metrics.accuracy_score(y_test, y_hat)\n",
    "jc2=jaccard_score(y_test, y_hat)\n",
    "fs2=f1_score(y_test, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "The Random Forrest improves on the advantages and disadvantages of Decision Trees. It is called Random Forest because it operates by constructing a multitude of Decision Trees on various, random sub-samples of the dataset and then outputs the class that is the mode of the classes.  The number of trees in the forest is set to 100 and the criterion to 'entropy' ensure the same method of information gain is used as before. Overall, the Random Forrest produced attained a general accuracy of 0.5870, slightly higher than that of single Decision Tree. Fortunately, this means that the previous Decision Tree model was very close to the precision of the Random Forrest, but inversely this suggests that a much higher accuracy with both these models cannot be attained on the current training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC=RandomForestClassifier(n_estimators=100, criterion=\"entropy\")\n",
    "RFC.fit(x_train, y_train)\n",
    "\n",
    "y_hat = RFC.predict(x_test)\n",
    "\n",
    "# full evaluation\n",
    "acc3=metrics.accuracy_score(y_test, y_hat)\n",
    "jc3=jaccard_score(y_test, y_hat)\n",
    "fs3=f1_score(y_test, y_hat, average='weighted')\n",
    "print(\"Random Forest General Accuracy: \", acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another crucial ability of Random Forest is 'feature importance scoring' (see bar graph below).  The higher a feature is scored, the more important the feature is in the total reduction of entropy and thus finding pure leaves (the total score must equal 1). By far, the two most important features in predicting collision severity on Seattle’s streets are the continuous variables `LATITUDE` and `LONGUTIDE`. At much lower importance, they are followed by whether a collision happens either `At Interaction (Intersection related)` or happens `Mid-Block (not related to intersection)`. On place four and five of importance scoring are `Unknown Roadcond` and `Unknown Weather` (features that cannot be translated into actions for improving traffic safety). Other features such as `WEEKEND, LIGHTCOND, INATTENTION, SPEEDING,` and others that could be turned into actionable insights for improving traffic safety, have an almost insignificant importance score in predicting collision severity. This confirms the apprehensions about the large size of the feature set and the large number of dummy variables as postulated in section 3.5. The implications of this ranking in feature importance will also be discussed in the next chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing feature importance top 10\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "\n",
    "feature_imp = pd.Series(RFC.feature_importances_, index=x.columns).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp[:10], y=feature_imp.index[:10])\n",
    "\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "In statistics and machine learning, Logistic Regression is employed not only to model binary classification as pass/fail, win/lose, or healthy/sick, but also the probability of a case falling into either class. Logistic Regression analysis is an alternative method to linear regression. Whereas linear regression tries to predict a continuous outcome by finding a linear equation, Logistic Regression does not look at the relationship between target and features as a straight line. Instead, Logistic Regression uses the natural logarithm function to find the relationship among the features that separate the targets into classes. In order to minimize the error of fitting this function to the training dataset, several solvers can be used such as `liblinear, SAG,` and `SAGA`.\n",
    "\n",
    "`Liblinear` is a library for large linear classification that support logistic regression and linear support vector machines.  A linear classifier model works by making a classification based on the value of the linear combination of the feature values. `Liblinear` is recommended for large-scale and high-dimension datasets, same as is given here. It uses coordinate descent which successively approximates minimization. Stochastic Average Gradient or `SAG` minimizes the sum of error on a smooth convex line. It is also considered a fast solver for large datasets, when both the number of samples and the number of features are high.  The `SAGA` solver is a variant of `SAG` that supports the non-smooth error minimization. It is recommended for multi-class classification.  All solvers also need regularization parameter C (to prevent over-fitting). The value of C is inverse to its regularization strength; i.e. smaller values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build logistic regression \"liblinear\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear')\n",
    "LR.fit(x_train,y_train)\n",
    "\n",
    "y_hat = LR.predict(x_test)\n",
    "print(\"Logistic Regression (liblinear) General Accuracy: \", metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build logistic regression \"sag\"\n",
    "LR = LogisticRegression(C=0.01, solver='sag')\n",
    "LR.fit(x_train,y_train)\n",
    "\n",
    "y_hat = LR.predict(x_test)\n",
    "print(\"Logistic Regression (sag) General Accuracy: \", metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build logistic regression \"saga\"\n",
    "\n",
    "LR = LogisticRegression(C=0.01, solver='saga')\n",
    "LR.fit(x_train,y_train)\n",
    "\n",
    "y_hat = LR.predict(x_test)\n",
    "print(\"Logistic Regression (saga) General Accuracy: \", metrics.accuracy_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite applying different solvers to the training data in order to classify collision severity, all three Logistic Regression models have attained an identical general accuracy score around 0.62. As the `Liblinear` solver seems the most appropriate for the type of dataset that is given here, it will be used for the full evaluation later. Additionally, since Logistic Regression can also estimate probability of class, it will also be included in the full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full evaluation\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear')\n",
    "LR.fit(x_train,y_train)\n",
    "yhat_prob = LR.predict_proba(x_test)\n",
    "\n",
    "acc4=metrics.accuracy_score(y_test, y_hat)\n",
    "jc4=jaccard_score(y_test, y_hat)\n",
    "fs4=f1_score(y_test, y_hat, average='weighted')\n",
    "ll4=log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network\n",
    "An Artificial Neural Network is constituted of nodes that are called artificial neurons, which loosely mirror the neurons in a biological brain. Each connection between nodes, like the synapses in a biological brain, can transmit a signal to other neurons. In Neural Networks (NNs), neurons receive a number of signals or input data, they perform some mathematical function on that data, and then output a signal to the next neuron. To transfer their signal outputs to other neurons, neurons have connections named edges and neurons are aggregated into layers. Signals can traverse from the first layer to the last layer multiple times and thus also recreate the feedback mechanism of biological neurons.\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg \"Artificial Neural Network Structure\")\n",
    "\n",
    "An NN classifier trains itself by iterating through the original input data and comparing the final output (i.e. the prediction) with a given target label.  As for any other ML algorithm, the difference between predicted and true label is the error. With successive iterations through the data, the NN adjusts the functions of neurons and layers, which will cause the NN to converge on predicted outputs with minimal difference to the true target. Here, the number of iterations is set at 200 and solver is set for `adam`.  Similar to Logistic Regression, NN can also estimate the probability of a case falling into a dependent class. The general accuracy score attained by artificial NN is 0.6243."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build neural network classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "NNC = MLPClassifier(max_iter=200, solver='adam')\n",
    "NNC.fit(x_train, y_train)\n",
    "y_hat = NNC.predict(x_test)\n",
    "\n",
    "# full evaluation\n",
    "yhat_proba = NNC.predict_proba(x_test)\n",
    "\n",
    "acc5=metrics.accuracy_score(y_test, y_hat)\n",
    "jc5=jaccard_score(y_test, y_hat)\n",
    "fs5=f1_score(y_test, y_hat, average='weighted')\n",
    "ll5=log_loss(y_test, yhat_proba)\n",
    "\n",
    "print(\"Artificial NN General Accuracy:\", acc5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Formal Evaluation Metrics\n",
    "Various evaluation metrics can be used to assess and compare the performance of the ML classification models developed in the previous chapter. All accuracy scores in tables below are obtained by testing the models on the previously created testing subset to simulate their performance on out-of-sample cases. The simplest accuracy score, titled General Accuracy before, is the number of all correct predictions over the total number of samples. It also can be read as the percentage of how many predictions were correct of all prediction that were made. All classification models tested have an average General Accuracy score 60.2% and they all fall within one standard deviation of 2.2%. Except for the Decision Tree, which is within 2 standard deviations below the mean, illustrating the worst performing model in terms of General Accuracy.\n",
    "\n",
    "The Jaccard Score, also known as Jaccard similarity coefficient is a metric for calculating the dissimilarity between two sample sets, i.e. the predicted classes and the actual classes. The Jaccard Score is defined as the size of the intersection divided by the size of the union of the two sample sets. It can also be read as percentage; the higher the values, the higher the overlap between the samples. The classification models developed have an average Jaccard score of 42.5% and fall within one standard deviation of 1.8%, except for the NN. The NN is within 2 standard deviations above the mean, showing  better performance than the other models in terms of Jaccard Score\n",
    "\n",
    "While the General Accuracy and Jaccard-Score are relatively simple metrics of hit or miss, the F1-Score is more sophisticated accuracy measure because it measures the balance between true positive and false positives.  The F1-score is the harmonic mean of the precision and recall (ranging between 0 for worst and 1 for best). Precision is the number of true positive results divided by the number of all positive results, including false positives. Recall is the number of true positive results divided by the number of all samples that should have been identified as positive. 3 out of 5 classification models fall within one standard deviation of the mean of 0.602. Again, the decision tree performed worse (within 2 standard deviations below the mean), while the artificial NN performed best (within 2 standard deviations above the mean).\n",
    "\n",
    "Logistic Regressions and artificial NNs can estimate the probability of a case falling in either of the dependent classes on top of their binary classification capabilities. A common metric the assess the uncertainty of the predicted probabilities is logistic loss (also called cross-entropy loss), abbreviated as Log-Loss. Log-Loss scores read in the opposite direction as the previous accuracy metrics. For any given problem, a lower Log-Loss value means better predictions as it means lower uncertainty. The NN model did slightly better than the Logistic Regression model by a difference of 0.002 in terms of Log-Loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a report table\n",
    "list_acc = [acc1, acc2, acc3, acc4, acc5]\n",
    "list_jc = [jc1, jc2, jc3, jc4, jc5]\n",
    "list_fs = [fs1, fs2, fs3, fs4, fs5]\n",
    "list_ll = [np.NaN, np.NaN, np.NaN, ll4, ll5]\n",
    "\n",
    "# fomulate the report format\n",
    "df = pd.DataFrame(list_acc, index=['K-Nearest Neighbor','Decision Tree','Random Forest','Logistic Regression','Neural Network'])\n",
    "df.columns = ['General Accuracy']\n",
    "df.insert(loc=1, column='Jaccard-Score', value=list_jc)\n",
    "df.insert(loc=2, column='F1-score', value=list_fs)\n",
    "df.insert(loc=3, column='Log-Loss', value=list_ll)\n",
    "df.columns.name = 'Algorithm'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Classifier and Best Determinants of Collision Severity?\n",
    "\n",
    "In conclusion, the artificial NN performs best in all evaluation categories. However, it is not a winner by a magnitude only by a margin. There is very little overall variation in the performance of all models and no single model significantly standouts as better, despite their very different approaches. This confirms the negative rather than the positive apprehensions formulated in section 3.5 about the lack of numerical features and abundance of dummy ones. To recall, the classification model was hypothesized to be a function where reckless human behavior and adverse driving conditions increase the severity of collisions. The abundance of dummy features has led to only moderate accuracy for all models. Moreover, the feature importance scoring has demonstrated that most conditions -that would make good policy-levers for improving traffic safety- had almost no significance in determining the classification of severity (see section 4.3). Ultimately, the top three best models for predicting collision severity on the streets of Seattle have been:\n",
    "1.\tArtificial Neural Networks\n",
    "2.\tLogistic Regression\n",
    "3.\tK-Nearest Neighbors "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
